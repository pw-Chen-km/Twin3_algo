"""
GemmaModelManager - çµ±ä¸€çš„ Gemma æ¨¡å‹ç®¡ç†å™¨
è² è²¬è¼‰å…¥å’Œç®¡ç† Gemma æ¨¡å‹å¯¦ä¾‹ï¼Œæ”¯æŒå¤šæ¨¡çµ„é–“å…±äº«ä»¥ç¯€çœè¨˜æ†¶é«”
"""

import os
import torch

# Gemma-3 å¤šæ¨¡æ…‹ LLM æ•´åˆ
try:
    from transformers import AutoProcessor, AutoModelForImageTextToText
    import torch
    GEMMA_AVAILABLE = True
except ImportError:
    print("è­¦å‘Šï¼štransformersæœªå®‰è£ï¼ŒGemmaModelManagerå°‡ä½¿ç”¨æ¨¡æ“¬æ¨¡å¼")
    GEMMA_AVAILABLE = False
    torch = None

class GemmaModelManager:
    """çµ±ä¸€çš„ Gemma æ¨¡å‹ç®¡ç†å™¨ï¼Œæ”¯æŒå¤šæ¨¡çµ„é–“å…±äº«"""
    
    _instance = None  # å–®ä¾‹æ¨¡å¼
    _model_loaded = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not hasattr(self, 'initialized'):
            self.initialized = True
            self.gemma_processor = None
            self.gemma_model = None
            self.device = None
            self.is_available = GEMMA_AVAILABLE
            self.model_path = None
    
    def load_model(self, 
                   gemma_model_name: str = "google/gemma-3n-E4B-it",
                   local_model_path: str = "models/gemma-3n-E4B-it",
                   use_local: bool = True) -> bool:
        """
        è¼‰å…¥ Gemma æ¨¡å‹ï¼ˆå¦‚æœå°šæœªè¼‰å…¥ï¼‰
        è¿”å›æ˜¯å¦æˆåŠŸè¼‰å…¥
        """
        if self._model_loaded:
            print(f"âœ… Gemma æ¨¡å‹å·²è¼‰å…¥ï¼Œé‡è¤‡ä½¿ç”¨ä¸­")
            return True
        
        if not self.is_available:
            print("âŒ transformers ä¸å¯ç”¨ï¼Œç„¡æ³•è¼‰å…¥ Gemma æ¨¡å‹")
            return False
        
        try:
            # ç¢ºå®šä½¿ç”¨çš„æ¨¡å‹è·¯å¾‘
            if use_local and local_model_path and os.path.exists(local_model_path):
                model_path = local_model_path
                print(f"ğŸ  æ­£åœ¨è¼‰å…¥æœ¬åœ° Gemma æ¨¡å‹: {model_path}")
            elif use_local and local_model_path:
                print(f"âŒ æ‰¾ä¸åˆ°æœ¬åœ°æ¨¡å‹: {local_model_path}")
                print("è«‹ç¢ºèªæ¨¡å‹å·²ä¸‹è¼‰åˆ°æ­£ç¢ºä½ç½®ï¼Œæˆ–ä½¿ç”¨ç·šä¸Šæ¨¡å¼")
                raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨: {local_model_path}")
            else:
                model_path = gemma_model_name
                print(f"ğŸŒ æ­£åœ¨è¼‰å…¥ç·šä¸Š Gemma æ¨¡å‹: {model_path}")
            
            self.model_path = model_path
            
            # è¼‰å…¥ Processor
            print("ğŸ“¦ è¼‰å…¥ Processor...")
            self.gemma_processor = AutoProcessor.from_pretrained(model_path)
            
            # è¨­ç½®è¨­å‚™å„ªå…ˆé †åº
            import torch
            if torch.backends.mps.is_available():
                self.device = torch.device("mps")
                print("ğŸš€ ä½¿ç”¨ MPS (Apple Silicon) åŠ é€Ÿ")
            elif torch.cuda.is_available():
                self.device = torch.device("cuda")
                print(f"ğŸš€ ä½¿ç”¨ CUDA GPU åŠ é€Ÿ")
            else:
                self.device = torch.device("cpu")
                print("ğŸ’» ä½¿ç”¨ CPU é‹ç®—")

            # è¼‰å…¥ Gemma å¤šæ¨¡æ…‹æ¨¡å‹ï¼ˆä½¿ç”¨æ­£ç¢ºçš„æ¨¡å‹é¡åˆ¥ï¼‰
            print("ğŸ¤– è¼‰å…¥ Gemma å¤šæ¨¡æ…‹æ¨¡å‹...")
            self.gemma_model = AutoModelForImageTextToText.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16 if self.device.type != "cpu" else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None
            )
            
            # å¦‚æœä¸æ˜¯ä½¿ç”¨ device_mapï¼Œæ‰‹å‹•ç§»å‹•åˆ°è¨­å‚™
            if not torch.cuda.is_available():
                self.gemma_model = self.gemma_model.to(self.device)
            
            self.gemma_model.eval()
            
            self._model_loaded = True
            print(f"âœ… Gemma æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
            print(f"ğŸ“Š è¨­å‚™: {self.device}")
            print(f"ğŸ”§ æ¨¡å‹è·¯å¾‘: {model_path}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Gemma æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            self._model_loaded = False
            return False
    
    def get_model_components(self):
        """
        ç²å–æ¨¡å‹çµ„ä»¶ï¼ˆprocessor, model, deviceï¼‰
        å¦‚æœæ¨¡å‹æœªè¼‰å…¥ï¼Œè¿”å› None
        """
        if not self._model_loaded or not self.is_available:
            return None, None, None
        
        return self.gemma_processor, self.gemma_model, self.device
    
    def is_model_available(self) -> bool:
        """æª¢æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        return self._model_loaded and self.is_available
    
    def get_model_info(self) -> dict:
        """ç²å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "loaded": self._model_loaded,
            "available": self.is_available,
            "device": self.device,
            "model_path": self.model_path
        }

# å…¨å±€å–®ä¾‹å¯¦ä¾‹
gemma_manager = GemmaModelManager() 