"""
MSMM (Multi-Semantic Matching Module) - Gemini 2.5 Flash ç‰ˆæœ¬
------------------------------------------------------------
ä¸»è¦æ”¹å‹•
â–¸ ç§»é™¤ Gemma æœ¬åœ°æ¨¡å‹ï¼Œæ”¹ç”¨ Google Gemini 2.5 Flash ç·šä¸Š API
â–¸ ä¿ç•™ Sentence-BERT å‘é‡æ¯”å°æµç¨‹èˆ‡å›é€€è¦å‰‡
â–¸ æ”¯æ´æ–‡å­— / åœ–ç‰‡ï¼ˆinline bytes æˆ– URLï¼‰å¤šæ¨¡æ…‹è¼¸å…¥
ä¾è³´å¥—ä»¶
    pip install -U google-genai sentence-transformers scikit-learn pillow requests numpy
ç’°å¢ƒè®Šæ•¸
    export GEMINI_API_KEY="YOUR_GEMINI_API_KEY"
"""

from __future__ import annotations
import os, json, re, io, warnings, mimetypes
from typing import List, Dict, Tuple
import base64
import requests
from PIL import Image

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# Google Gemini SDK
from google import genai
from google.genai import types


# ---------------------------  ä¸»é¡åˆ¥  --------------------------- #
class MSMMProcessor:
    def __init__(
        self,
        metadata_path: str = "metadata/attribute_metadata.json",
        gemini_model_name: str = "gemini-2.5-flash",
    ):
        """åˆå§‹åŒ– MSMM è™•ç†å™¨ï¼ˆGemini ç‰ˆï¼‰"""
        # 1) è¼‰å…¥å±¬æ€§ç¶­åº¦å®šç¾©
        self.metadata_path = metadata_path
        self.metadata = self._load_metadata()

        # 2) å¤šèª Sentence-BERT
        try:
            self.sentence_model = SentenceTransformer(
                "paraphrase-multilingual-MiniLM-L12-v2"
            )
        except Exception:
            warnings.warn("âš ï¸ ç„¡æ³•è¼‰å…¥å¤šèª MiniLMï¼Œæ”¹ç”¨è‹±æ–‡æ¨¡å‹")
            self.sentence_model = SentenceTransformer("all-MiniLM-L6-v2")

        # 3) Google Gemini Client
        self.gemini_client = genai.Client(api_key="AIzaSyASSZjukErHRK7fZfbhBZEfNECUR7C1bdc")
        self.gemini_model_name = gemini_model_name

        # 4) é å…ˆè¨ˆç®— attribute å‘é‡
        self._precompute_attribute_vectors()

    # ---------------------------------------------------------- #
    #                         è¼”åŠ©å‡½å¼
    # ---------------------------------------------------------- #
    def _load_metadata(self) -> Dict:
        try:
            with open(self.metadata_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            warnings.warn(f"âŒ æ‰¾ä¸åˆ° {self.metadata_path}")
            return {}

    def _precompute_attribute_vectors(self):
        self.attribute_vectors = {}
        for attr_id, attr_data in self.metadata.items():
            vec = self.sentence_model.encode(
                [" ".join(attr_data["attribute_meta_tags"])]
            )[0]
            self.attribute_vectors[attr_id] = vec
        print(f"âœ… å·²é è¨ˆç®— {len(self.attribute_vectors)} å€‹ç¶­åº¦å‘é‡")

    # ------------------------ Gemini èƒå– ----------------------- #
    def _build_prompt(self, user_content: str) -> str:
        # ç³»çµ± + ä»»å‹™æŒ‡ä»¤åˆä½µç‚ºå–®ä¸€æ–‡å­— prompt
        all_tags = {tag for v in self.metadata.values() for tag in v["attribute_meta_tags"]}
        sample_tags = ", ".join(list(all_tags)[:30])

        prompt = f"""
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å…§å®¹åˆ†æ AIï¼Œéœ€è¦å¾ç”¨æˆ¶çš„ç”Ÿæ´»é«”é©—å…§å®¹ä¸­æå–é—œéµçš„ Meta-Tagsã€‚
ä»»å‹™ï¼šåˆ†æä»¥ä¸‹ç”¨æˆ¶å…§å®¹ï¼Œæå– 3â€“8 å€‹æœ€èƒ½ä»£è¡¨å…¶æ ¸å¿ƒæ„ç¾©çš„ Meta-Tagsã€‚
ç”¨æˆ¶å…§å®¹ï¼šã€Œ{user_content}ã€

æå–æŒ‡å°ï¼š
1. Meta-Tags æ‡‰ç‚ºç°¡æ½”çš„ä¸­æ–‡è©å½™æˆ–è‹±æ–‡å–®è©
2. èšç„¦è¡Œç‚ºã€æƒ…ç·’ã€å ´æ™¯ã€æŠ€èƒ½ã€åƒ¹å€¼è§€ç­‰æ ¸å¿ƒæ¦‚å¿µ
3. å¦‚æœ‰åœ–ç‰‡ï¼Œçµåˆåœ–ç‰‡å…§å®¹
4. å¯åƒè€ƒä½†ä¸é™æ–¼ï¼š{sample_tags}
5. é¿å…éæ–¼å…·é«”çš„åè©ï¼Œå‚¾å‘å¯é‡è¤‡ä½¿ç”¨çš„æ¦‚å¿µ

è«‹**åƒ…è¼¸å‡º**ä»¥é€—è™Ÿåˆ†éš”çš„ Meta-Tagsï¼Œç„¡å…¶ä»–æ–‡å­—ã€‚
ä¾‹å¦‚ï¼šå­¸ç¿’, æˆå°±æ„Ÿ, åœ˜éšŠåˆä½œ, é£Ÿç‰©, æ…¶ç¥
""".strip()
        return prompt

    def _image_part_from_url_or_path(self, image_url: str) -> types.Part | None:
        try:
            if image_url.startswith(("http://", "https://")):
                img_bytes = requests.get(image_url, timeout=10).content
                mime = requests.head(image_url, timeout=10).headers.get("Content-Type", "image/jpeg")
            else:
                with open(image_url, "rb") as f:
                    img_bytes = f.read()
                mime, _ = mimetypes.guess_type(image_url)
                mime = mime or "image/jpeg"
            return types.Part.from_bytes(data=img_bytes, mime_type=mime)
        except Exception as e:
            warnings.warn(f"âš ï¸ åœ–ç‰‡è®€å–å¤±æ•—ï¼š{e}")
            return None

    def _gemini_extract_tags(self, user_content: str, image_url: str | None) -> List[str]:
        prompt = self._build_prompt(user_content)
        parts: List[types.Part | str] = []

        # åœ–ç‰‡è‹¥æœ‰æˆåŠŸè½‰ç‚º Partï¼Œæ¡ç”¨ã€Œåœ–ç‰‡ + promptã€æ’åˆ—
        if image_url:
            img_part = self._image_part_from_url_or_path(image_url)
            if img_part:
                parts.append(img_part)
        parts.append(prompt)

        try:
            response = self.gemini_client.models.generate_content(
                model=self.gemini_model_name,
                contents=parts if len(parts) > 1 else parts[0],
                # é—œé–‰ thinking ä»¥ç¯€çœæˆæœ¬/å»¶é²ï¼ˆå¯è‡ªè¡Œèª¿æ•´ï¼‰
                config=types.GenerateContentConfig(
                    thinking_config=types.ThinkingConfig(thinking_budget=0)
                ),
            )  #  [oai_citation:0â€¡Google AI for Developers](https://ai.google.dev/gemini-api/docs/quickstart)
            resp_text = response.text.strip()
        except Exception as e:
            warnings.warn(f"âš ï¸ Gemini API å¤±æ•—ï¼š{e}")
            return self._fallback_meta_tag_extraction(user_content)

        # é€—è™Ÿ / é “è™Ÿåˆ‡åˆ†
        tags = re.split(r"[,\sã€]+", resp_text)
        tags = [t.strip() for t in tags if 1 < len(t) < 20][:8]
        return tags if tags else self._fallback_meta_tag_extraction(user_content)

    # ---------------------------------------------------------- #
    #                   å›é€€ï¼šç°¡æ˜“è¦å‰‡èƒå–
    # ---------------------------------------------------------- #
    def _fallback_meta_tag_extraction(self, text: str) -> List[str]:
        text_l = text.lower()
        rule = {
            "achievement": ["å®Œæˆ", "æˆå°±", "æˆåŠŸ", "é”æˆ", "ç²å¾—"],
            "leadership": ["å¸¶é ˜", "é ˜å°", "æŒ‡å°", "ä¸»æŒ"],
            "food": ["åƒ", "é£Ÿç‰©", "é¤å»³", "æ–™ç†", "æ—©é¤", "åˆé¤", "æ™šé¤"],
            "learning": ["å­¸ç¿’", "ç ”ç©¶", "é–±è®€", "èª²ç¨‹", "çŸ¥è­˜"],
            "creative": ["å‰µä½œ", "è¨­è¨ˆ", "è—è¡“", "éŸ³æ¨‚", "æ”å½±"],
            "exercise": ["é‹å‹•", "å¥èº«", "è·‘æ­¥", "æ¸¸æ³³", "ç‘œä¼½"],
            "social": ["æœ‹å‹", "èšæœƒ", "ç¤¾äº¤", "åœ˜é«”", "å¿—å·¥"],
            "technology": ["ç¨‹å¼", "ç§‘æŠ€", "è»Ÿé«”", "é›»è…¦", "ai"],
            "environment": ["æ°¸çºŒ", "ç’°ä¿", "æ¸›ç¢³", "ç¶ è‰²", "ç”Ÿæ…‹"],
        }
        hits = [k for k, kw in rule.items() if any(w in text_l for w in kw)]
        if hits:
            return list(set(hits))
        # æ²’å‘½ä¸­ â†’ æŠ½å‰å¹¾å€‹è©
        words = re.findall(r"[\u4e00-\u9fff]{2,}|[a-zA-Z]{2,}", text)
        return words[:5]

    # ---------------------------------------------------------- #
    #        å°å¤– APIï¼šæå– Meta-Tags + æ‰¾åŒ¹é…ç¶­åº¦
    # ---------------------------------------------------------- #
    def extract_meta_tags(self, user_content: str, image_url: str | None = None) -> List[str]:
        tags = self._gemini_extract_tags(user_content, image_url)
        # ä¿å­˜æœ€å¾Œæå–çš„ Meta-Tags ä¾›é‡ç”¨
        self._last_extracted_tags = tags
        return tags

    def find_matching_attributes(
        self, user_content: str, image_url: str | None = None, top_n: int = 5
    ) -> List[Tuple[str, float, str]]:
        """è¨ˆç®—å…§å®¹èˆ‡å„ attribute çš„ç›¸ä¼¼åº¦ï¼Œå›å‚³ Top-N"""
        user_tags = self.extract_meta_tags(user_content, image_url)
        print("ğŸ¯ èƒå– Meta-Tags:", user_tags)
        if not user_tags:
            return []

        # æ‰¹æ¬¡çµ„ pair
        user_batch, attr_batch, mapping = [], [], []
        for ut in user_tags:
            for aid, adata in self.metadata.items():
                for at in adata["attribute_meta_tags"]:
                    user_batch.append(ut)
                    attr_batch.append(at)
                    mapping.append(aid)

        # å‘é‡åŒ–èˆ‡ cos ç›¸ä¼¼åº¦
        u_vecs = self.sentence_model.encode(user_batch)
        a_vecs = self.sentence_model.encode(attr_batch)
        sims = cosine_similarity(u_vecs, a_vecs).diagonal()

        # èšåˆåˆ° attribute
        score_sum, pair_cnt = {}, {}
        for aid, s in zip(mapping, sims):
            score_sum[aid] = score_sum.get(aid, 0) + s
            pair_cnt[aid] = pair_cnt.get(aid, 0) + 1
        avg_scores = {aid: score_sum[aid] / pair_cnt[aid] for aid in score_sum}

        ranked = sorted(
            ((aid, sc, self.metadata[aid]["attribute_name"]) for aid, sc in avg_scores.items()),
            key=lambda x: x[1],
            reverse=True,
        )
        return ranked[:top_n]

    def process_user_content(
        self, user_content: str, image_url: str | None = None, threshold: float = 0.5
    ) -> List[Dict]:
        """ä¸»å…¥å£ï¼šè¿”å›ç›¸ä¼¼åº¦ â‰¥ threshold çš„åŒ¹é…ç¶­åº¦"""
        top_attrs = self.find_matching_attributes(user_content, image_url)
        res = [
            {
                "attribute_id": aid,
                "attribute_name": name,
                "similarity_score": score,
            }
            for aid, score, name in top_attrs
            if score >= threshold
        ]
        print("âœ… åŒ¹é…çµæœ:", res or "ç„¡")
        return res


# ---------------------------  å¿«é€Ÿæ¸¬è©¦  --------------------------- #
def _quick_test():
    msmm = MSMMProcessor()
    text = "æˆ‘ä»Šå¤©å¸¶é ˜å­¸å¼Ÿå¦¹å®Œæˆäº†ä¸€ç¯‡è«–æ–‡ï¼Œç„¶å¾Œå»åƒäº†å°å¼æ—©é¤æ…¶ç¥ï¼"
    return msmm.process_user_content(text)


if __name__ == "__main__":
    _quick_test()