"""
DADEE (Dynamic Attribute Development & Evolution Engine)
ä¿ƒé€²Twin Matrixçš„å‹•æ…‹æ¼”é€²ï¼Œåæ˜ ç¤¾ç¾¤çš„å…±è­˜è®ŠåŒ–
"""

import json
import os
import re
from typing import List, Dict, Set, Tuple
from collections import Counter
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

class DADEEProcessor:
    def __init__(self, 
                 data_path: str = None,
                 metadata_path: str = None,
                 similarity_threshold: float = 0.6,
                 min_cluster_size: int = 2):
        """åˆå§‹åŒ–DADEEè™•ç†å™¨"""
        # è‡ªå‹•åµæ¸¬æ­£ç¢ºçš„è·¯å¾‘
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(current_dir)  # å‡è¨­ src/ åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹
        
        if data_path is None:
            # å˜—è©¦æ‰¾åˆ° data ç›®éŒ„
            if os.path.exists(os.path.join(project_root, "data")):
                self.data_path = os.path.join(project_root, "data") + "/"
            elif os.path.exists("data"):
                self.data_path = "data/"
            else:
                self.data_path = "../data/"
        else:
            self.data_path = data_path
            
        if metadata_path is None:
            # å˜—è©¦æ‰¾åˆ° metadata ç›®éŒ„
            if os.path.exists(os.path.join(project_root, "metadata", "attribute_metadata.json")):
                self.metadata_path = os.path.join(project_root, "metadata", "attribute_metadata.json")
            elif os.path.exists("metadata/attribute_metadata.json"):
                self.metadata_path = "metadata/attribute_metadata.json"
            else:
                self.metadata_path = "../metadata/attribute_metadata.json"
        else:
            self.metadata_path = metadata_path
            
        self.similarity_threshold = similarity_threshold
        self.min_cluster_size = min_cluster_size
        
        self.metadata = self._load_metadata()
        
        # åˆå§‹åŒ–Sentence-BERTæ¨¡å‹
        try:
            self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        except:
            print("è­¦å‘Šï¼šç„¡æ³•è¼‰å…¥å¤šèªè¨€æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¤è‹±æ–‡æ¨¡å‹")
            self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # é å…ˆè¨ˆç®—ç¾æœ‰ç¶­åº¦çš„å‘é‡
        self._precompute_existing_vectors()
    
    def _load_metadata(self) -> Dict:
        """è¼‰å…¥ç¶­åº¦å…ƒæ•¸æ“š"""
        try:
            with open(self.metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°å…ƒæ•¸æ“šæ–‡ä»¶ {self.metadata_path}")
            return {}
    
    def _precompute_existing_vectors(self):
        """é å…ˆè¨ˆç®—æ‰€æœ‰ç¾æœ‰ç¶­åº¦çš„å‘é‡è¡¨ç¤º"""
        self.existing_vectors = {}
        self.existing_meta_tags = []
        
        for attr_id, attr_data in self.metadata.items():
            meta_tags = attr_data.get("attribute_meta_tags", [])
            self.existing_meta_tags.extend(meta_tags)
            
            # ç‚ºæ•´å€‹ç¶­åº¦è¨ˆç®—å‘é‡
            combined_text = " ".join([
                attr_data.get("attribute_name", ""),
                " ".join(meta_tags),
                attr_data.get("definition", "")
            ])
            
            vector = self.sentence_model.encode([combined_text])
            self.existing_vectors[attr_id] = vector[0]
        
        # å»é‡ç¾æœ‰Meta-Tags
        self.existing_meta_tags = list(set(self.existing_meta_tags))
        print(f"å·²è¼‰å…¥ {len(self.metadata)} å€‹ç¾æœ‰ç¶­åº¦ï¼ŒåŒ…å« {len(self.existing_meta_tags)} å€‹Meta-Tags")
    
    def extract_content_meta_tags_from_files(self) -> List[str]:
        """å¾æ‰€æœ‰æ•¸æ“šæ–‡ä»¶ä¸­æå–å…§å®¹Meta-Tagsï¼ˆèˆŠç‰ˆæ–¹æ³•ï¼Œä¿ç•™å‘å¾Œå…¼å®¹ï¼‰"""
        all_meta_tags = []
        
        # éæ­·dataç›®éŒ„ä¸‹çš„æ‰€æœ‰txtæ–‡ä»¶
        if not os.path.exists(self.data_path):
            print(f"éŒ¯èª¤ï¼šæ•¸æ“šç›®éŒ„ {self.data_path} ä¸å­˜åœ¨")
            return []
        
        txt_files = [f for f in os.listdir(self.data_path) if f.endswith('.txt')]
        print(f"æ‰¾åˆ° {len(txt_files)} å€‹æ•¸æ“šæ–‡ä»¶")
        
        for filename in txt_files:
            filepath = os.path.join(self.data_path, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    
                # æå–Meta-Tagsï¼ˆä½¿ç”¨ç°¡åŒ–çš„é—œéµè©æå–ï¼‰
                meta_tags = self._extract_meta_tags_from_content(content)
                all_meta_tags.extend(meta_tags)
                
            except Exception as e:
                print(f"è®€å–æ–‡ä»¶ {filename} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        print(f"ç¸½å…±æå–äº† {len(all_meta_tags)} å€‹Meta-Tags")
        return all_meta_tags
    
    def extract_meta_tags_from_records(self, records_path: str = None) -> List[str]:
        """å¾ç”¨æˆ¶Meta-Tagè¨˜éŒ„ä¸­æå–æ‰€æœ‰Meta-Tagsï¼ˆæ–°ç‰ˆä¸»è¦æ–¹æ³•ï¼‰"""
        all_meta_tags = []
        
        # å¦‚æœæ²’æœ‰æŒ‡å®šè·¯å¾‘ï¼Œè‡ªå‹•åµæ¸¬
        if records_path is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(current_dir)
            
            # å˜—è©¦ä¸åŒçš„å¯èƒ½è·¯å¾‘
            possible_paths = [
                os.path.join(project_root, "state", "user_metatags_records.json"),
                "state/user_metatags_records.json",
                "../state/user_metatags_records.json"
            ]
            
            records_path = None
            for path in possible_paths:
                if os.path.exists(path):
                    records_path = path
                    break
            
            if records_path is None:
                print("âŒ æ‰¾ä¸åˆ°Meta-Tagè¨˜éŒ„æ–‡ä»¶ï¼Œå˜—è©¦éçš„è·¯å¾‘ï¼š")
                for path in possible_paths:
                    print(f"  - {path}")
                print("âš ï¸  è«‹å…ˆé‹è¡Œmain.pyè™•ç†ä¸€äº›ç”¨æˆ¶å…§å®¹ä»¥ç”Ÿæˆè¨˜éŒ„")
                return []
        
        try:
            with open(records_path, 'r', encoding='utf-8') as f:
                records = json.load(f)
            
            print(f"è¼‰å…¥äº† {len(records)} å€‹ç”¨æˆ¶çš„Meta-Tagè¨˜éŒ„")
            
            # çµ±è¨ˆæ‰€æœ‰ç”¨æˆ¶çš„meta-tag
            tag_frequency = {}
            total_tags = 0
            
            for user_id, user_tags in records.items():
                print(f"  {user_id}: {len(user_tags)} å€‹ä¸åŒæ¦‚å¿µ")
                
                for tag, info in user_tags.items():
                    count = info.get('count', 1)
                    total_tags += count
                    
                    # ç´¯ç©æ¯å€‹tagçš„ç¸½å‡ºç¾æ¬¡æ•¸
                    if tag in tag_frequency:
                        tag_frequency[tag] += count
                    else:
                        tag_frequency[tag] = count
                    
                    # å°‡tagæŒ‰å‡ºç¾æ¬¡æ•¸æ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼ˆç”¨æ–¼èšé¡åˆ†æï¼‰
                    all_meta_tags.extend([tag] * count)
            
            print(f"ç¸½å…±ç™¼ç¾ {len(tag_frequency)} å€‹ä¸åŒçš„Meta-Tagæ¦‚å¿µ")
            print(f"ç¸½è¨ˆ {total_tags} æ¬¡Meta-Tagå‡ºç¾")
            
            # é¡¯ç¤ºæœ€é »ç¹çš„æ¦‚å¿µ
            top_tags = sorted(tag_frequency.items(), key=lambda x: x[1], reverse=True)[:10]
            print("æœ€å¸¸å‡ºç¾çš„æ¦‚å¿µï¼š")
            for tag, freq in top_tags:
                print(f"  ğŸ·ï¸  {tag}: {freq} æ¬¡")
            
            return all_meta_tags
            
        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°Meta-Tagè¨˜éŒ„æ–‡ä»¶: {records_path}")
            print("âš ï¸  è«‹å…ˆé‹è¡Œmain.pyè™•ç†ä¸€äº›ç”¨æˆ¶å…§å®¹ä»¥ç”Ÿæˆè¨˜éŒ„")
            return []
        except json.JSONDecodeError:
            print(f"âŒ Meta-Tagè¨˜éŒ„æ–‡ä»¶æ ¼å¼éŒ¯èª¤: {records_path}")
            return []
        except Exception as e:
            print(f"âŒ è®€å–Meta-Tagè¨˜éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
    
    def analyze_user_metatag_trends(self, records_path: str = None) -> Dict:
        """åˆ†æç”¨æˆ¶Meta-Tagè¶¨å‹¢å’Œåˆ†ä½ˆ"""
        # å¦‚æœæ²’æœ‰æŒ‡å®šè·¯å¾‘ï¼Œè‡ªå‹•åµæ¸¬
        if records_path is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(current_dir)
            
            # å˜—è©¦ä¸åŒçš„å¯èƒ½è·¯å¾‘
            possible_paths = [
                os.path.join(project_root, "state", "user_metatags_records.json"),
                "state/user_metatags_records.json",
                "../state/user_metatags_records.json"
            ]
            
            records_path = None
            for path in possible_paths:
                if os.path.exists(path):
                    records_path = path
                    break
            
            if records_path is None:
                print("âŒ æ‰¾ä¸åˆ°Meta-Tagè¨˜éŒ„æ–‡ä»¶é€²è¡Œè¶¨å‹¢åˆ†æ")
                return {}
        
        try:
            with open(records_path, 'r', encoding='utf-8') as f:
                records = json.load(f)
            
            analysis = {
                "total_users": len(records),
                "concept_distribution": {},
                "user_activity": {},
                "temporal_trends": {},
                "emerging_concepts": []
            }
            
            # åˆ†ææ¦‚å¿µåˆ†ä½ˆ
            concept_freq = {}
            user_concept_counts = {}
            
            for user_id, user_tags in records.items():
                user_concept_counts[user_id] = len(user_tags)
                
                for tag, info in user_tags.items():
                    count = info.get('count', 1)
                    if tag in concept_freq:
                        concept_freq[tag] += count
                    else:
                        concept_freq[tag] = count
            
            analysis["concept_distribution"] = dict(sorted(concept_freq.items(), key=lambda x: x[1], reverse=True))
            analysis["user_activity"] = user_concept_counts
            
            # è­˜åˆ¥æ–°èˆˆæ¦‚å¿µï¼ˆé »æ¬¡é«˜ä½†ç”¨æˆ¶åˆ†æ•£åº¦ä¹Ÿé«˜çš„æ¦‚å¿µï¼‰
            for tag, total_freq in concept_freq.items():
                user_count = sum(1 for user_tags in records.values() if tag in user_tags)
                if total_freq >= 3 and user_count >= 2:  # è‡³å°‘3æ¬¡å‡ºç¾ä¸”è‡³å°‘2å€‹ç”¨æˆ¶
                    analysis["emerging_concepts"].append({
                        "tag": tag,
                        "frequency": total_freq,
                        "user_count": user_count,
                        "diversity_score": user_count / len(records)
                    })
            
            # æŒ‰å¤šæ¨£æ€§åˆ†æ•¸æ’åºæ–°èˆˆæ¦‚å¿µ
            analysis["emerging_concepts"].sort(key=lambda x: x["diversity_score"], reverse=True)
            
            return analysis
            
        except Exception as e:
            print(f"åˆ†æç”¨æˆ¶Meta-Tagè¶¨å‹¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {}
    
    def _extract_meta_tags_from_content(self, content: str) -> List[str]:
        """å¾å–®å€‹å…§å®¹ä¸­æå–Meta-Tags"""
        content_lower = content.lower()
        
        # ç’°ä¿ç›¸é—œé—œéµè©
        environmental_keywords = [
            "æ°¸çºŒ", "ç’°ä¿", "æ¸›ç¢³", "ç¶ è‰²", "ç”Ÿæ…‹", "æ°£å€™", "åƒåœ¾åˆ†é¡", 
            "æœ‰æ©Ÿ", "å†ç”Ÿèƒ½æº", "å¤ªé™½èƒ½", "é¢¨åŠ›", "æ¸…æ½”èƒ½æº", "æ·¨ç˜",
            "æ±¡æŸ“", "ä¿è­·", "é›¶å»¢æ£„", "å¯é‡è¤‡åˆ©ç”¨", "ç”Ÿç‰©å¤šæ¨£æ€§",
            "æ°¸çºŒç™¼å±•", "ç¢³è¶³è·¡", "ç’°å¢ƒä¿è­·", "ç¶ è‰²ç§‘æŠ€"
        ]
        
        # å…¶ä»–é¡åˆ¥é—œéµè©
        other_keywords = {
            "social": ["ç¤¾å€", "å¿—å·¥", "æ…ˆå–„", "å¹«åŠ©", "ç¤¾æœƒ", "å…¬ç›Š"],
            "learning": ["å­¸ç¿’", "ç ”ç©¶", "é–±è®€", "èª²ç¨‹", "æŠ€å·§", "çŸ¥è­˜", "è¬›åº§"],
            "technology": ["ç¨‹å¼", "ç§‘æŠ€", "è»Ÿé«”", "é›»è…¦", "AI", "æ•¸ä½", "å‰µæ–°"],
            "food": ["åƒ", "é£Ÿç‰©", "é¤å»³", "æ–™ç†", "æ—©é¤", "ç«é‹", "å¸‚é›†"],
            "leadership": ["å¸¶é ˜", "é ˜å°", "æŒ‡å°", "ç®¡ç†", "çµ„ç¹”"],
            "achievement": ["å®Œæˆ", "æˆå°±", "æˆåŠŸ", "ç²å¾—", "å¯¦ç¾"]
        }
        
        extracted_tags = []
        
        # æª¢æŸ¥ç’°ä¿ç›¸é—œé—œéµè©
        for keyword in environmental_keywords:
            if keyword in content_lower:
                extracted_tags.append(keyword)
        
        # æª¢æŸ¥å…¶ä»–é¡åˆ¥é—œéµè©
        for category, keywords in other_keywords.items():
            for keyword in keywords:
                if keyword in content_lower:
                    extracted_tags.append(keyword)
        
        # æå–ä¸­æ–‡è©å½™
        chinese_words = re.findall(r'[\u4e00-\u9fff]+', content)
        for word in chinese_words:
            if len(word) >= 2 and word not in extracted_tags:
                extracted_tags.append(word)
        
        return extracted_tags
    
    def perform_clustering_analysis(self, meta_tags: List[str]) -> List[List[str]]:
        """å°Meta-Tagsé€²è¡Œèšé¡åˆ†æ"""
        if len(meta_tags) < self.min_cluster_size:
            print("Meta-Tagsæ•¸é‡ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œèšé¡åˆ†æ")
            return []
        
        # çµ±è¨ˆè©é »ï¼Œéæ¿¾ä½é »è©
        tag_counts = Counter(meta_tags)
        frequent_tags = [tag for tag, count in tag_counts.items() if count >= 2]
        
        if len(frequent_tags) < self.min_cluster_size:
            print("é«˜é »Meta-Tagsæ•¸é‡ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œèšé¡åˆ†æ")
            return []
        
        print(f"ä½¿ç”¨ {len(frequent_tags)} å€‹é«˜é »Meta-Tagsé€²è¡Œèšé¡åˆ†æ")
        
        # å‘é‡åŒ–
        tag_vectors = self.sentence_model.encode(frequent_tags)
        
        # ä½¿ç”¨DBSCANé€²è¡Œèšé¡
        clustering = DBSCAN(
            eps=0.3,  # è¼ƒå°çš„epså€¼ä»¥ç²å¾—æ›´ç·Šå¯†çš„èšé¡
            min_samples=self.min_cluster_size,
            metric='cosine'
        ).fit(tag_vectors)
        
        # çµ„ç¹”èšé¡çµæœ
        clusters = {}
        for i, label in enumerate(clustering.labels_):
            if label != -1:  # å¿½ç•¥å™ªé»
                if label not in clusters:
                    clusters[label] = []
                clusters[label].append(frequent_tags[i])
        
        cluster_list = list(clusters.values())
        print(f"ç™¼ç¾ {len(cluster_list)} å€‹èªæ„èšé¡")
        
        return cluster_list
    
    def evaluate_novelty(self, clusters: List[List[str]]) -> List[Dict]:
        """è©•ä¼°æ–°æ¦‚å¿µçš„æ–°ç©æ€§"""
        novel_concepts = []
        
        for i, cluster in enumerate(clusters):
            cluster_text = " ".join(cluster)
            cluster_vector = self.sentence_model.encode([cluster_text])[0]
            
            # è¨ˆç®—èˆ‡æ‰€æœ‰ç¾æœ‰ç¶­åº¦çš„ç›¸ä¼¼åº¦
            max_similarity = 0.0
            most_similar_attr = ""
            
            for attr_id, existing_vector in self.existing_vectors.items():
                similarity = cosine_similarity([cluster_vector], [existing_vector])[0][0]
                if similarity > max_similarity:
                    max_similarity = similarity
                    most_similar_attr = attr_id
            
            # åˆ¤æ–·æ˜¯å¦ç‚ºæ–°ç©æ¦‚å¿µ
            if max_similarity < self.similarity_threshold:
                novel_concepts.append({
                    'cluster_id': i,
                    'meta_tags': cluster,
                    'max_similarity': max_similarity,
                    'most_similar_attribute': most_similar_attr,
                    'novelty_score': 1.0 - max_similarity
                })
        
        # æŒ‰æ–°ç©æ€§è©•åˆ†æ’åº
        novel_concepts.sort(key=lambda x: x['novelty_score'], reverse=True)
        
        return novel_concepts
    
    def generate_evolution_proposals(self, novel_concepts: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆç¶­åº¦æ¼”é€²ææ¡ˆ"""
        proposals = []
        
        for concept in novel_concepts:
            meta_tags = concept['meta_tags']
            novelty_score = concept['novelty_score']
            most_similar_attr = concept['most_similar_attribute']
            
            # åˆ†ææ¦‚å¿µä¸»é¡Œ
            concept_theme = self._analyze_concept_theme(meta_tags)
            
            # ç”Ÿæˆå»ºè­°
            if novelty_score > 0.6:  # é«˜æ–°ç©æ€§ï¼Œå»ºè­°å‰µå»ºæ–°ç¶­åº¦
                proposal = {
                    'type': 'create_new_attribute',
                    'concept_theme': concept_theme,
                    'suggested_name': self._suggest_attribute_name(meta_tags),
                    'meta_tags': meta_tags,
                    'novelty_score': novelty_score,
                    'rationale': f"ç™¼ç¾é«˜æ–°ç©æ€§æ¦‚å¿µï¼ˆè©•åˆ†: {novelty_score:.3f}ï¼‰ï¼Œå»ºè­°å‰µå»ºæ–°ç¶­åº¦"
                }
            else:  # ä¸­ç­‰æ–°ç©æ€§ï¼Œå»ºè­°æ“´å±•ç¾æœ‰ç¶­åº¦
                similar_attr_name = self.metadata.get(most_similar_attr, {}).get('attribute_name', 'Unknown')
                proposal = {
                    'type': 'enhance_existing_attribute',
                    'target_attribute_id': most_similar_attr,
                    'target_attribute_name': similar_attr_name,
                    'concept_theme': concept_theme,
                    'suggested_additional_tags': meta_tags,
                    'novelty_score': novelty_score,
                    'rationale': f"èˆ‡ç¶­åº¦ {most_similar_attr} ç›¸ä¼¼åº¦ç‚º {concept['max_similarity']:.3f}ï¼Œå»ºè­°æ“´å±•å…¶Meta-Tags"
                }
            
            proposals.append(proposal)
        
        return proposals
    
    def _analyze_concept_theme(self, meta_tags: List[str]) -> str:
        """åˆ†ææ¦‚å¿µä¸»é¡Œ"""
        # ç’°ä¿ä¸»é¡Œæª¢æ¸¬
        environmental_tags = ["æ°¸çºŒ", "ç’°ä¿", "æ¸›ç¢³", "ç¶ è‰²", "ç”Ÿæ…‹", "æ°£å€™", "æœ‰æ©Ÿ", "å†ç”Ÿèƒ½æº"]
        if any(tag in meta_tags for tag in environmental_tags):
            return "ç’°å¢ƒä¿è­·èˆ‡æ°¸çºŒç™¼å±•"
        
        # ç¤¾æœƒè²¬ä»»ä¸»é¡Œæª¢æ¸¬
        social_tags = ["ç¤¾å€", "å¿—å·¥", "æ…ˆå–„", "å¹«åŠ©", "å…¬ç›Š"]
        if any(tag in meta_tags for tag in social_tags):
            return "ç¤¾æœƒè²¬ä»»èˆ‡å…¬æ°‘åƒèˆ‡"
        
        # ç§‘æŠ€ä¸»é¡Œæª¢æ¸¬
        tech_tags = ["ç§‘æŠ€", "æ•¸ä½", "AI", "å‰µæ–°", "ç¨‹å¼"]
        if any(tag in meta_tags for tag in tech_tags):
            return "ç§‘æŠ€æ‡‰ç”¨èˆ‡å‰µæ–°"
        
        return "å…¶ä»–æ–°èˆˆæ¦‚å¿µ"
    
    def _suggest_attribute_name(self, meta_tags: List[str]) -> str:
        """å»ºè­°ç¶­åº¦åç¨±"""
        # åŸºæ–¼Meta-Tagså»ºè­°åç¨±
        if any(tag in ["æ°¸çºŒ", "ç’°ä¿", "æ¸›ç¢³", "ç¶ è‰²"] for tag in meta_tags):
            return "Environmental Consciousness"
        elif any(tag in ["ç¤¾å€", "å¿—å·¥", "æ…ˆå–„"] for tag in meta_tags):
            return "Community Engagement"
        elif any(tag in ["ç§‘æŠ€", "å‰µæ–°", "æ•¸ä½"] for tag in meta_tags):
            return "Digital Innovation"
        else:
            return "Emerging Life Dimension"
    
    def run_evolution_analysis(self, use_records: bool = True, records_path: str = None) -> Dict:
        """åŸ·è¡Œå®Œæ•´çš„ç¶­åº¦æ¼”é€²åˆ†æ"""
        print("\n=== DADEE ç¶­åº¦æ¼”é€²åˆ†æ ===")
        
        # 1. æ•¸æ“šèšåˆ
        if use_records:
            print("æ­¥é©Ÿ1: å¾ç”¨æˆ¶Meta-Tagè¨˜éŒ„ä¸­æå–è³‡æ–™...")
            all_meta_tags = self.extract_meta_tags_from_records(records_path)
            
            if not all_meta_tags:
                print("âš ï¸  æ²’æœ‰æ‰¾åˆ°Meta-Tagè¨˜éŒ„ï¼Œå˜—è©¦å¾èˆŠç‰ˆæ–‡ä»¶è®€å–...")
                all_meta_tags = self.extract_content_meta_tags_from_files()
        else:
            print("æ­¥é©Ÿ1: å¾ç”¨æˆ¶æ•¸æ“šæ–‡ä»¶ä¸­æå–Meta-Tags...")
            all_meta_tags = self.extract_content_meta_tags_from_files()
        
        if not all_meta_tags:
            print("æ²’æœ‰æ‰¾åˆ°å¯åˆ†æçš„æ•¸æ“š")
            return {'proposals': [], 'status': 'no_data'}
        
        # 2. èšé¡åˆ†æ
        print("æ­¥é©Ÿ2: åŸ·è¡Œèšé¡åˆ†æ...")
        clusters = self.perform_clustering_analysis(all_meta_tags)
        
        if not clusters:
            print("æ²’æœ‰ç™¼ç¾èªæ„èšé¡")
            return {'proposals': [], 'status': 'no_clusters'}
        
        # 3. æ–°ç©æ€§è©•ä¼°
        print("æ­¥é©Ÿ3: è©•ä¼°æ¦‚å¿µæ–°ç©æ€§...")
        novel_concepts = self.evaluate_novelty(clusters)
        
        if not novel_concepts:
            print("æ²’æœ‰ç™¼ç¾æ–°ç©æ¦‚å¿µ")
            return {'proposals': [], 'status': 'no_novel_concepts'}
        
        # 4. ç”Ÿæˆææ¡ˆ
        print("æ­¥é©Ÿ4: ç”Ÿæˆæ¼”é€²ææ¡ˆ...")
        proposals = self.generate_evolution_proposals(novel_concepts)
        
        return {
            'proposals': proposals,
            'clusters_found': len(clusters),
            'novel_concepts_found': len(novel_concepts),
            'status': 'success'
        }
    
    def run_user_trend_analysis(self) -> Dict:
        """åŸ·è¡Œç”¨æˆ¶è¶¨å‹¢åˆ†æï¼ˆåŸºæ–¼Meta-Tagè¨˜éŒ„ï¼‰"""
        print("\n=== DADEE ç”¨æˆ¶è¶¨å‹¢åˆ†æ ===")
        
        analysis = self.analyze_user_metatag_trends()
        
        if not analysis:
            print("ç„¡æ³•åŸ·è¡Œè¶¨å‹¢åˆ†æ")
            return {}
        
        print(f"åˆ†æäº† {analysis['total_users']} å€‹ç”¨æˆ¶çš„Meta-Tagè¨˜éŒ„")
        
        # é¡¯ç¤ºæ–°èˆˆæ¦‚å¿µ
        emerging = analysis.get('emerging_concepts', [])
        if emerging:
            print(f"\nç™¼ç¾ {len(emerging)} å€‹æ–°èˆˆæ¦‚å¿µï¼š")
            for concept in emerging[:5]:  # é¡¯ç¤ºå‰5å€‹
                print(f"  ğŸŒ± {concept['tag']}: {concept['frequency']} æ¬¡, {concept['user_count']} ç”¨æˆ¶, å¤šæ¨£æ€§ {concept['diversity_score']:.3f}")
        
        # é¡¯ç¤ºç”¨æˆ¶æ´»èºåº¦
        activity = analysis.get('user_activity', {})
        if activity:
            avg_concepts = sum(activity.values()) / len(activity)
            print(f"\nç”¨æˆ¶å¹³å‡æ¦‚å¿µæ•¸é‡: {avg_concepts:.1f}")
            most_active = max(activity.items(), key=lambda x: x[1])
            print(f"æœ€æ´»èºç”¨æˆ¶: {most_active[0]} ({most_active[1]} å€‹æ¦‚å¿µ)")
        
        return analysis
    
    def print_proposals(self, analysis_results: Dict):
        """æ ¼å¼åŒ–æ‰“å°ææ¡ˆ"""
        proposals = analysis_results.get('proposals', [])
        
        if not proposals:
            print("æ²’æœ‰ç”Ÿæˆä»»ä½•æ¼”é€²ææ¡ˆ")
            return
        
        print(f"\n=== DADEE æ¼”é€²ææ¡ˆå ±å‘Š ===")
        print(f"ç™¼ç¾ {len(proposals)} å€‹æ¼”é€²æ©Ÿæœƒ\n")
        
        for i, proposal in enumerate(proposals, 1):
            print(f"ææ¡ˆ {i}: {proposal['type']}")
            print(f"æ¦‚å¿µä¸»é¡Œ: {proposal['concept_theme']}")
            print(f"æ–°ç©æ€§è©•åˆ†: {proposal['novelty_score']:.3f}")
            
            if proposal['type'] == 'create_new_attribute':
                print(f"å»ºè­°æ–°ç¶­åº¦åç¨±: {proposal['suggested_name']}")
                print(f"å»ºè­°Meta-Tags: {proposal['meta_tags']}")
            else:
                print(f"ç›®æ¨™ç¶­åº¦: {proposal['target_attribute_id']}-{proposal['target_attribute_name']}")
                print(f"å»ºè­°æ–°å¢Meta-Tags: {proposal['suggested_additional_tags']}")
            
            print(f"ç†ç”±: {proposal['rationale']}")
            print("-" * 60)

# æ¸¬è©¦å‡½æ•¸
def test_dadee():
    """æ¸¬è©¦DADEEæ¨¡çµ„"""
    dadee = DADEEProcessor()
    results = dadee.run_evolution_analysis()
    dadee.print_proposals(results)
    return results

if __name__ == "__main__":
    test_dadee() 